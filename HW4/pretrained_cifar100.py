# -*- coding: utf-8 -*-
"""Resnet_Pretrain.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t9HB5LyZZcso3U8EQ26TpomKAypNHWXn
"""

import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.autograd import Variable
import torchvision.transforms as transforms
import numpy as np
from torch.utils import model_zoo


# import data and data augmentation
transform_train = transforms.Compose(
    [
     transforms.RandomHorizontalFlip(),
     #transforms.RandomRotation(20),
     #transforms.Resize(40),
     #transforms.RandomGrayscale(p=0.1),
     #transforms.CenterCrop(224),
     transforms.ToTensor(),
     transforms.Normalize((0.507, 0.487, 0.441), (0.267, 0.256, 0.276))])
transform_test = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.507, 0.487, 0.441), (0.267, 0.256, 0.276))])
trainset = torchvision.datasets.CIFAR100(root='./data', train=True,
                                        download=True, transform=transform_train)
traindataloader = torch.utils.data.DataLoader(trainset, batch_size=128,
                                          shuffle=True)
testset = torchvision.datasets.CIFAR100(root='./data', train=False,
                                       download=True, transform=transform_test)
testdataloader = torch.utils.data.DataLoader(testset, batch_size=128,
                                         shuffle=False)


model_urls = {
    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',
}

def resnet18(pretrained=True) :
    model = torchvision.models.resnet.ResNet(torchvision.models.resnet.BasicBlock, [2, 2, 2, 2])
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls['resnet18'], model_dir = './pretrained'))
    return model
  
model = resnet18()
in_features = model.fc.in_features
model.fc = nn.Linear(in_features, 100)
    
#model.unsample = nn.Upsample(scale_factor=7, mode=’bilinear’)

model.cuda()

# define optimization method
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-6)

#Train Model
model.train()
batch_size = 128
num_epochs = 100
for epoch in range(num_epochs):  
    train_accu = []
    for i, data in enumerate(traindataloader, 0):
        x_train, y_train = data
        data, labels = Variable(x_train).cuda(), Variable(y_train).cuda()
        data = F.upsample(data, scale_factor=7, mode='bilinear') #upsample the data to fit the size 
        optimizer.zero_grad()
        model.train()
        outputs = model(data)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        prediction = outputs.data.max(1)[1]  
        accuracy = ( float( prediction.eq(labels.data).sum() ) /float(batch_size) )*100.0
        train_accu.append(accuracy)
    accuracy_epoch = np.mean(train_accu)
    print(epoch,accuracy_epoch)
    test_accu = []
    for i, data in enumerate(testdataloader, 0):
      x_test, y_test = data
      data1, labels1 = Variable(x_test).cuda(), Variable(y_test).cuda()
      data1 = F.upsample(data1, scale_factor=7, mode='bilinear')  #upsample the data to fit the size 
      optimizer.zero_grad()
      model.eval()
      outputs1 = model(data1)     
      prediction = outputs1.data.max(1)[1]  
      accuracy = ( float( prediction.eq(labels1.data).sum() ) /float(batch_size) )*100.0
      test_accu.append(accuracy)
    accuracy = np.mean(test_accu)
    print('test_accu:',accuracy)