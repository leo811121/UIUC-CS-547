# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zwfU977RhSHAfwCaeRnxrR7gYvn-Phzv
"""

import torch
import torch.nn as nn
import torch.utils.data as Data
import torchvision
import torchvision.transforms as transforms
import torch.optim as optim
import os
import numpy as np
from torch.autograd import Variable

def Training_Method(cnn, test_dataloader, eval_bool):
  if (eval_bool == True):
    cnn.eval()
  else:
    cnn.train()

  test_accu_test = []
  for step, (x_test,y_test) in enumerate(test_dataloader):
    b_x_test = Variable(x_test).cuda()
    b_y_test = Variable(y_test).cuda()
    optimizer.zero_grad()
    output_test = cnn(b_x_test)
    prediction_test = output_test.data.max(1)[1].cuda()
    accuracy_test = ( float( prediction_test.eq(b_y_test.data).sum() ) /100.0 )
    test_accu_test.append(accuracy_test)
  accuracy_epoch_test = np.mean(test_accu_test)
  return accuracy_epoch_test

  
# Data Augmentation

train_data_transform = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.49, 0.48, 0.44), (0.2, 0.2, 0.2))
])

test_data_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.49, 0.48, 0.44), (0.2, 0.2, 0.2))
])

# Data Loader

train_dataset = torchvision.datasets.CIFAR10(
    root = './',
    train = True,
    download = True,
    transform = train_data_transform
)

train_dataloader = Data.DataLoader(
    train_dataset,
    batch_size = 128,
    shuffle = True,
    num_workers = 2
)

test_dataset = torchvision.datasets.CIFAR10(
    root = './',
    train = False,
    download = True,
    transform = test_data_transform
)

test_dataloader = Data.DataLoader(
    test_dataset,
    batch_size = 100,
    shuffle = True,
    num_workers = 2
)


# Model
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Sequential(
            nn.Conv2d(
                in_channels = 3,
                out_channels = 64,
                kernel_size = 4,
                stride = 1,
                padding = 2
            ),
            nn.BatchNorm2d(64),
        )
        
        self.conv2 = nn.Sequential(
            nn.Conv2d(64, 64, 4, 1, 2),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size = 2, stride = 2),
            #nn.Dropout(p = 0.005),
            nn.Dropout(p = 0.05),
        )
        
        self.conv3 = nn.Sequential(
            nn.Conv2d(64, 64, 4, 1, 2),
            nn.BatchNorm2d(64),
            nn.ReLU(),
        )
        
        self.conv4 = nn.Sequential(
            nn.Conv2d(64, 64, 4, 1, 2),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size = 2, stride = 2),
            #nn.Dropout(p = 0.005),
            nn.Dropout(p = 0.05),
        )
        
        self.conv5 = nn.Sequential(
            nn.Conv2d(64, 64, 4, 1, 2),
            nn.BatchNorm2d(64),
            nn.ReLU(),
        )
        
        self.conv6 = nn.Sequential(
            nn.Conv2d(64, 64, 3, 1, 0),
            nn.ReLU(),
            #nn.Dropout(p = 0.005),
            nn.Dropout(p = 0.05),
        )
        
        self.conv7 = nn.Sequential(
            nn.Conv2d(64, 64, 3, 1, 0),
            nn.BatchNorm2d(64),
            nn.ReLU(),
        )
        
        self.conv8 = nn.Sequential(
            nn.Conv2d(64, 64, 3, 1, 0),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            #nn.Dropout(p = 0.005),
            nn.Dropout(p = 0.05),
        )
        
        self.fc1 = nn.Sequential(
            nn.Linear(1024,500),
            nn.ReLU(500)
        ) 
        
        self.fc2 = nn.Sequential(
            nn.Linear(500,10)
        )
        
    def forward(self, x):
        #print('shape',x.size())
        x = self.conv1(x)
        #print('conv1')
        #print('shape',x.size())
        x = self.conv2(x)
        #print('conv2')
        #print('shape',x.size())
        x = self.conv3(x)
        #print('conv3')
        #print('shape',x.size())
        x = self.conv4(x)
        #print('conv4')
        #print('shape',x.size())
        x = self.conv5(x)
        #print('conv5')
        #print('shape',x.size())
        x = self.conv6(x)
        #print('conv6')
        #print('shape',x.size())
        x = self.conv7(x)
        #print('conv7')
        #print('shape',x.size())
        x = self.conv8(x)
        #print('conv8')
        #print('shape',x.size())
        x = x.view(x.size(0),-1)
        #print('view')
        #print('shape',x.size())
        x = self.fc1(x)
        #print('fc1')
        x = self.fc2(x)
        return x

cnn = CNN()
cnn.train()
cnn.cuda()

optimizer = torch.optim.Adam(cnn.parameters(), lr = 0.01)
loss_func = nn.CrossEntropyLoss()

Epochs = 40
for epoch in range(Epochs):
  train_accu = []     #modify
  for step, (x,y) in enumerate(train_dataloader):
      b_x = Variable(x).cuda()
      b_y = Variable(y).cuda()
      optimizer.zero_grad()
      output = cnn(b_x)
      loss = loss_func(output, b_y)
      loss.backward()
      optimizer.step()

      prediction = output.data.max(1)[1].cuda()
      accuracy = ( float( prediction.eq(b_y.data).sum() ) /128.0 )
      train_accu.append(accuracy)
  accuracy_epoch = np.mean(train_accu)
  print('epoch:', epoch, 'train accuracy', accuracy_epoch)
  
  
  #training
  if (accuracy_epoch > 0.88):
  
    #cnn.train
    Train_normal = Training_Method(cnn, test_dataloader, False)
    print('test accuracy with cnn.train()',Train_normal)

    #Heurictic Method
    Train_Heruistic = Training_Method(cnn, test_dataloader, True)
    print('test accuracy by Heurictic Method',Train_Heruistic)

    #Monte Carlo method
    Monte_accu = []
    for i in range(100):
      Monte_time = Training_Method(cnn, test_dataloader, False)
      Monte_accu.append(Monte_time)
    Train_Monte = np.mean(Monte_accu)
    print('test accuracy by Monte Carlo Method',Train_Monte)
  
    if(Train_normal>0.8 and Train_Heruistic>0.8 and Train_Monte>0.8):
      break





